{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout,BatchNormalization,Conv2D,MaxPool2D\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"F:/Computers/Python/EmotionsRecognitionModel/emotions.h5\")\n",
    "\n",
    "class_labels = {0: 'Angry',1: 'Fear',2: 'Happy',3: 'Neutral',4: 'Sad',5: 'Surprise'}\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier(\"C:/Users/Abhinav/Face-Recognition-master/Haarcascades/haarcascade_frontalface_default.xml\") # CHANGE THE LOCATION TO THE PATH OF HAR CASCADE FILE\n",
    "\n",
    "def face_detector(img):\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48),np.uint8), img\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        y = y-20\n",
    "        h = h+50\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48),np.uint8),img\n",
    "    return (x,y,w,h),roi_gray,img\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret,frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    (x,y,w,h),face,image = face_detector(frame)\n",
    "    \n",
    "    if np.sum([face]) != 0.0:\n",
    "        \n",
    "        roi = face.astype('float') / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        preds = model.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]\n",
    "        \n",
    "        cv2.putText(image, label, (x,y), cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        cv2.putText(image,\"No face found\", (20,60), cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "\n",
    "    cv2.imshow(\"Emotions\",image)\n",
    "    \n",
    "    if cv2.waitKey(1)==13:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
